C:\Users\52893\anaconda3\envs\torch\python.exe H:/研究生/研一论文/图像处理/图像融合/code/transformer/SwinFuse-main/test.py
SSIM weight ----- 1e1
SwinTransformerSys expand initial----depths:[6, 6, 6];drop_path_rate:0.2;out_chans:1
SwinFuse----PatchEmbed-> PatchEmbed(
  (proj): Conv2d(1, 96, kernel_size=(1, 1), stride=(1, 1))
  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
)
num_patches-> 50176
patches_resolution-> [224, 224]
dpr-> [0.0, 0.0117647061124444, 0.0235294122248888, 0.03529411926865578, 0.0470588244497776, 0.05882352963089943, 0.07058823853731155, 0.08235294371843338, 0.0941176488995552, 0.10588235408067703, 0.11764705926179886, 0.12941177189350128, 0.1411764770746231, 0.15294118225574493, 0.16470588743686676, 0.1764705926179886, 0.1882352977991104, 0.20000000298023224]
-----------StageModule-------------
dim-> 96
input_resolution-> (224, 224)
depth-> 6
use_checkpoint-> False
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 1
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 1
head_dim-> 96
scale-> 0.10206207261596575
relative_position_bias_table-> torch.Size([169, 1])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 1
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 1
head_dim-> 96
scale-> 0.10206207261596575
relative_position_bias_table-> torch.Size([169, 1])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
C:\Users\52893\anaconda3\envs\torch\lib\site-packages\torch\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\TensorShape.cpp:2895.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 1
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 1
head_dim-> 96
scale-> 0.10206207261596575
relative_position_bias_table-> torch.Size([169, 1])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 1
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 1
head_dim-> 96
scale-> 0.10206207261596575
relative_position_bias_table-> torch.Size([169, 1])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 1
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 1
head_dim-> 96
scale-> 0.10206207261596575
relative_position_bias_table-> torch.Size([169, 1])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 1
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 1
head_dim-> 96
scale-> 0.10206207261596575
relative_position_bias_table-> torch.Size([169, 1])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
-----------StageModule-------------
dim-> 96
input_resolution-> (224, 224)
depth-> 6
use_checkpoint-> False
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 2
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 2
head_dim-> 48
scale-> 0.14433756729740643
relative_position_bias_table-> torch.Size([169, 2])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 2
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 2
head_dim-> 48
scale-> 0.14433756729740643
relative_position_bias_table-> torch.Size([169, 2])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 2
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 2
head_dim-> 48
scale-> 0.14433756729740643
relative_position_bias_table-> torch.Size([169, 2])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 2
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 2
head_dim-> 48
scale-> 0.14433756729740643
relative_position_bias_table-> torch.Size([169, 2])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 2
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 2
head_dim-> 48
scale-> 0.14433756729740643
relative_position_bias_table-> torch.Size([169, 2])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 2
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 2
head_dim-> 48
scale-> 0.14433756729740643
relative_position_bias_table-> torch.Size([169, 2])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
-----------StageModule-------------
dim-> 96
input_resolution-> (224, 224)
depth-> 6
use_checkpoint-> False
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 4
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 4
head_dim-> 24
scale-> 0.2041241452319315
relative_position_bias_table-> torch.Size([169, 4])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 4
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 4
head_dim-> 24
scale-> 0.2041241452319315
relative_position_bias_table-> torch.Size([169, 4])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 4
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 4
head_dim-> 24
scale-> 0.2041241452319315
relative_position_bias_table-> torch.Size([169, 4])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 4
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 4
head_dim-> 24
scale-> 0.2041241452319315
relative_position_bias_table-> torch.Size([169, 4])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 4
window_size-> 7
shift_size-> 0
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 4
head_dim-> 24
scale-> 0.2041241452319315
relative_position_bias_table-> torch.Size([169, 4])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
------------SwinTransformerBlock-----------------
dim-> 96
input_resolution-> (224, 224)
num_heads-> 4
window_size-> 7
shift_size-> 3
mlp_ratio-> 4.0
token_mlp-> ffn
-----------------WindowAttention--------------------------
dim-> 96
window_size-> (7, 7)
num_heads-> 4
head_dim-> 24
scale-> 0.2041241452319315
relative_position_bias_table-> torch.Size([169, 4])
coords_h.shapetorch.Size([7])   coords_w.shapetorch.Size([7])-> 
coords-> torch.Size([2, 7, 7])
coords_flatten-> torch.Size([2, 49])
relative_coords-> torch.Size([2, 49, 49])
relative_coords2-> torch.Size([49, 49, 2])
------------Mlp---------------
out_features-> 96
hidden_features-> 384
计算SW - MSA的注意力掩码
h224   w224
img_mask-> torch.Size([1, 224, 224, 1])
h_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
w_slices-> (slice(0, -7, None), slice(-7, -3, None), slice(-3, None, None))
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 1])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 1])
windows.shape-> torch.Size([1024, 7, 7, 1])
mask_windows219.shape-> torch.Size([1024, 7, 7, 1])
mask_windows221.shape-> torch.Size([1024, 49])
attn_mask.shape-> torch.Size([1024, 49, 49])
<generator object Module.parameters at 0x0000015F6FEFAF90>
2021276
Model SwinFuse : params: 8.085104M
paths_utils166 ['./data\\TNO/IR/1.png']
h_utils174 256
w_utils175 256
paths_utils166 ['./data\\TNO/VIS/1.png']
h_utils174 256
w_utils175 256
img_ir_temp.shape_test93-> torch.Size([1, 1, 224, 224])
img_vi_temp.shape_test94-> torch.Size([1, 1, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
tir1.shape_test95-> torch.Size([1, 96, 224, 224])
tvi1.shape_test96-> torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
tir3.shape_test120-> torch.Size([1, 96, 224, 224])
tvi3.shape_test121-> torch.Size([1, 96, 224, 224])
f.shape_test124-> torch.Size([1, 96, 224, 224])
-----------up_x4-----------------
x.shape615-> torch.Size([1, 1, 224, 224])
img_ir_temp.shape_test93-> torch.Size([1, 1, 224, 224])
img_vi_temp.shape_test94-> torch.Size([1, 1, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
tir1.shape_test95-> torch.Size([1, 96, 224, 224])
tvi1.shape_test96-> torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
tir3.shape_test120-> torch.Size([1, 96, 224, 224])
tvi3.shape_test121-> torch.Size([1, 96, 224, 224])
f.shape_test124-> torch.Size([1, 96, 224, 224])
-----------up_x4-----------------
x.shape615-> torch.Size([1, 1, 224, 224])
img_ir_temp.shape_test93-> torch.Size([1, 1, 224, 224])
img_vi_temp.shape_test94-> torch.Size([1, 1, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
tir1.shape_test95-> torch.Size([1, 96, 224, 224])
tvi1.shape_test96-> torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
tir3.shape_test120-> torch.Size([1, 96, 224, 224])
tvi3.shape_test121-> torch.Size([1, 96, 224, 224])
f.shape_test124-> torch.Size([1, 96, 224, 224])
-----------up_x4-----------------
x.shape615-> torch.Size([1, 1, 224, 224])
img_ir_temp.shape_test93-> torch.Size([1, 1, 224, 224])
img_vi_temp.shape_test94-> torch.Size([1, 1, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
----------encoder1-------------
x.shape541-> torch.Size([1, 1, 224, 224])
---------PatchEmbed------------
torch.Size([1, 1, 224, 224])
torch.Size([1, 50176, 96])
x.shape542-> torch.Size([1, 50176, 96])
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 1, 49, 96])
q-> torch.Size([1024, 1, 49, 96])
q2-> torch.Size([1024, 1, 49, 96])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x1，553----: torch.Size([1, 96, 224, 224])
tir1.shape_test95-> torch.Size([1, 96, 224, 224])
tvi1.shape_test96-> torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
-----------encoder2------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 2, 49, 48])
q-> torch.Size([1024, 2, 49, 48])
q2-> torch.Size([1024, 2, 49, 48])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------transformer577--------
------x2----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
----------encoder3--------------------
input_resolution.shape-> (224, 224)
x.shape-> torch.Size([1, 50176, 96])
-------------window_partition--------------------
x.shape-> torch.Size([1, 224, 224, 96])
x2.shape-> torch.Size([1, 32, 7, 32, 7, 96])
windows.shape-> torch.Size([1024, 7, 7, 96])
x_windows.shape249-> torch.Size([1024, 7, 7, 96])
x_windows.shape251-> torch.Size([1024, 49, 96])
x.shape-> torch.Size([1024, 49, 96])
qkv.shape-> torch.Size([3, 1024, 4, 49, 24])
q-> torch.Size([1024, 4, 49, 24])
q2-> torch.Size([1024, 4, 49, 24])
attn_windows.shape-> torch.Size([1024, 49, 96])
attn_windows.shape258-> torch.Size([1024, 7, 7, 96])
---------------------window_reverse----------------------------
B52-> 1
x.shape55-> torch.Size([1, 32, 32, 7, 7, 96])
x.shape57-> torch.Size([1, 224, 224, 96])
shifted_x.shape-> torch.Size([1, 224, 224, 96])
x.shape271-> torch.Size([1, 50176, 96])
x.shape275-> torch.Size([1, 50176, 96])
x1-> torch.Size([1, 50176, 96])
x2-> torch.Size([1, 50176, 384])
x3-> torch.Size([1, 50176, 384])
x4-> torch.Size([1, 50176, 384])
x5-> torch.Size([1, 50176, 96])
x6-> torch.Size([1, 50176, 96])
x.shape277-> torch.Size([1, 50176, 96])
x.shape338-> torch.Size([1, 50176, 96])
------x3----: torch.Size([1, 50176, 96])
------transformer577--------
-----x3result-----: torch.Size([1, 96, 224, 224])
tir3.shape_test120-> torch.Size([1, 96, 224, 224])
tvi3.shape_test121-> torch.Size([1, 96, 224, 224])
f.shape_test124-> torch.Size([1, 96, 224, 224])
-----------up_x4-----------------
x.shape615-> torch.Size([1, 1, 224, 224])
torch.Size([1, 256, 256])
(256, 256)
./outputs/fusion_1_swinfuse_0_f_type.png
Done......

进程已结束,退出代码0
